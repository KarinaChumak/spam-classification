{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oJ6jS0m-xPUg"
   },
   "source": [
    "This is test task for MMD Smart done by Karina Chumak.<br>\n",
    "Task: create spam/not-spam classificator using DL and clustering, provide exploratory data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Js4xtZS9xPUi"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "figsize = (15,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('data/processed_CDR_TEXT_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task CNN with two channels of word embeddings was implemented, one - static, one - non-static.\n",
    "This model architecture was described in Yoon Kim [paper](https://www.aclweb.org/anthology/D14-1181). I selected this model, because Weicheng Zhang, who compared LSTM and CNN in his [dissertation](http://weichengzhang.co/src/paper/Capstone_final_report.pdf), stated that CNN outperforms LSTM, even without word embeddings. Moreover, this architecture is quite simple,and performs remarkably well even with little hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "9AeJ2hCQxPWv",
    "outputId": "a257a055-7afd-419c-b872-84f2da34580a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model,Sequential\n",
    "from keras.layers import MaxPooling2D,Flatten,Convolution2D,Reshape,Concatenate,concatenate, Activation, Dense, Dropout, Input, Embedding\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint,TensorBoard\n",
    "from keras.initializers import Constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting data for train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ePWBsBhuxPWy"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X['CDR_TEXT'],X['labels'],test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using GloVe as word embeddings. The dimension of embedding was chosen as 50. Maximum number of words in embeddings is 200000, maximum length of sequence created from text is 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fVAhuh48-8QC"
   },
   "outputs": [],
   "source": [
    "GLOVE_FILE = 'data/glove.6B/glove.6B.50d.txt'\n",
    "MAX_SEQUENCE_LENGTH = 100 #the longest sequence is actually 89 words\n",
    "MAX_NUM_WORDS = 200000\n",
    "EMBEDDING_DIM = 50\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization, sequence forming, aequence padding\n",
    "tok = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tok.fit_on_texts(X_train)\n",
    "sequences = tok.texts_to_sequences(X_train)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1nVM5qNy_2z6"
   },
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "with open(GLOVE_FILE) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Bb1GXNXxPXC"
   },
   "outputs": [],
   "source": [
    "word_index = tok.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [],
    "colab": {},
    "colab_type": "code",
    "id": "WVsitfYIxPXE"
   },
   "outputs": [],
   "source": [
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_input1 = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "# static embedding layer\n",
    "embedded1 = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)(sequence_input1)\n",
    "# trainable embedding layer\n",
    "embedded2 = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)(sequence_input1)\n",
    "merged = concatenate([embedded1,embedded2],axis = -1)\n",
    "x = Reshape((2, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM))(merged)\n",
    "x = Convolution2D(64, 5, activation=\"relu\", data_format = \"channels_first\",border_mode='same')(x)\n",
    "x = MaxPooling2D(5)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(256, activation=\"relu\")(x)\n",
    "x = Dropout(0.3)(x)\n",
    "preds = Dense(2, activation=\"softmax\")(x)\n",
    "model = Model(inputs=sequence_input1, outputs=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sRaNghfqBhNE"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "                optimizer='rmsprop',\n",
    "                metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 100, 50)      6605650     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 100, 50)      6605650     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 100, 100)     0           embedding_3[0][0]                \n",
      "                                                                 embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 2, 100, 50)   0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 64, 100, 50)  3264        reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 12, 20, 50)   0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 12000)        0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          3072256     flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 256)          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 2)            514         dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 16,287,334\n",
      "Trainable params: 9,681,684\n",
      "Non-trainable params: 6,605,650\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "thHvlirUGTWd"
   },
   "outputs": [],
   "source": [
    "# save the weights on each epoch\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "# stop early if accuracy doesn't improve\n",
    "early_stop = EarlyStopping(monitor='val_loss',min_delta=0.0001)\n",
    "# define tensorboard for model diagnosis\n",
    "tensorboard = TensorBoard(log_dir='tensorboard/', histogram_freq=0,  \n",
    "          write_graph=True)\n",
    "callbacks_list = [checkpoint,early_stop,tensorboard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating one hot vector for labels\n",
    "from keras.utils import to_categorical\n",
    "y_binary = to_categorical(np.asarray(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "06rWU3nLBmtx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 630064 samples, validate on 157516 samples\n",
      "Epoch 1/5\n",
      "630064/630064 [==============================] - 14947s 24ms/step - loss: 0.0057 - acc: 0.9986 - val_loss: 0.0036 - val_acc: 0.9994\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.00567, saving model to weights-improvement-01-0.0057-bigger.hdf5\n",
      "Epoch 2/5\n",
      "630064/630064 [==============================] - 14965s 24ms/step - loss: 0.0035 - acc: 0.9993 - val_loss: 0.0041 - val_acc: 0.9994\n",
      "\n",
      "Epoch 00002: loss improved from 0.00567 to 0.00346, saving model to weights-improvement-02-0.0035-bigger.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd4582a7fd0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(sequences_matrix,y_binary,batch_size=128,epochs=5,\n",
    "          validation_split=0.2,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even after first epoch model had validation accuracy 99.94%, which did not impove after 2d epoch,so the model stopped early. Let's check how it works on test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NMlzLUTkxPXG"
   },
   "outputs": [],
   "source": [
    "test_sequences = tok.texts_to_sequences(X_test)\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_YBdsLzvxPXI"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(test_sequences_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the model is one-hot vector. The second column is pobability of being a spam. We'll use threshold of 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = (pd.Series(y_pred[:,1]) > 0.5) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PvwpSFMxxPXO",
    "outputId": "55ba1628-af05-4db4-a7a1-c93c6202e276"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN + 2 word embeddings\n",
      "Precision:0.9979421483491401\n",
      "Recall:0.997741017494995\n",
      "Accuracy:0.9992889647326507\n"
     ]
    }
   ],
   "source": [
    "prec = precision_score(Y_test,y_pred, average=\"macro\")\n",
    "rec = recall_score(Y_test,y_pred, average=\"macro\")\n",
    "acc = accuracy_score(Y_test,y_pred)\n",
    "print('CNN + 2 word embeddings')\n",
    "print('Precision:{}'.format(prec))\n",
    "print('Recall:{}'.format(rec))\n",
    "print('Accuracy:{}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the results are pretty good,taking to the account the simplicity of the model and absence of hyperparameter tuning. Let's build a confusion matrix to see, where our model makes mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pc2jpnXOxPXQ"
   },
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(Y_test, y_pred)\n",
    "conf_matrix_normalized = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R9dgiccQxPXS",
    "outputId": "20fcab75-4248-412d-fa78-4046d58a1ed2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 15.0, 'Predicted label')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(conf_matrix_normalized)\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sAg7vwJGxPXV",
    "outputId": "1764b80a-a188-4af5-8a38-5342daca7097"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[178996     66]\n",
      " [    74  17760]]\n"
     ]
    }
   ],
   "source": [
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>0 - normal, 1 - spam</b>\n",
    "<br>74 out of 17834 spam letters were marked as normal \n",
    "<br>66 out of 179062 normal letters were marked as spam\n",
    "<br> This model is making 1 type mistakes less, than 2 type, which is good, because marking normal letter as spam is more undesirable than the other way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('CNN_2-epoch.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "spam classification.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
